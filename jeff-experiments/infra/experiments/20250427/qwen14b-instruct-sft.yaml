#  sky launch -c jeff-cotdecomp-8xh100 jeff-experiments/infra/base_job.yaml  --down
resources:
  cloud: runpod
  accelerators: H100-SXM:4
  region: US
  disk_size: 2000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

file_mounts:
  ~/.ssh/id_rsa: ~/.ssh/id_rsa
  ~/.ssh/config: ~/.ssh/config
  ~/.netrc: ~/.netrc
  ~/.cache/huggingface/token: ~/.cache/huggingface/token
  ~/sky_workdir/verl: ~/verl
  ~/sky_workdir/cot-decomp: ~/cot-decomp
  ~/.env_anthropic: ~/.env_anthropic

# working dir is ~/sky_workdir/

envs:
  EXPERIMENT_NAME: mmlu-sft-qwen14b-instruct
  PROJECT_NAME: mmlu-sft
  REF_MODEL: Qwen/Qwen2.5-14B-Instruct
  FT_MODEL_NAME: Qwen2.5-14B-Instruct-SFT
  FT_TRAINING_STEPS: 204
  MICRO_BATCH_SIZE: 4

  EVAL_JSON_PATH: /root/sky_workdir/cot-decomp/experiments/notebooks/20250427-jeff/qwen_14b_instruct_sft.json
  EVAL_GRID_PATH: /root/sky_workdir/cot-decomp/experiments/notebooks/20250427-jeff/qwen_14b_instruct_sft_grid_results.json
  EVAL_OUTPUT_PATH: /root/sky_workdir/eval-output

setup: |
  set -e
  echo "Running setup."
  chown -R root ~/.ssh
  chmod -R 700 ~/.ssh
  rsync -e 'ssh -o "StrictHostKeyChecking no"' -av cluster1:/home/jeffg/mmlu_full_gpt41 ~/sky_workdir/
  ~/sky_workdir/verl/jeff-experiments/infra/base_setup.sh


# Remember to change lora_merge.py and fsdp_sft_trainer.py to use the correct model loader
# TODO(sguo35): rewrite this to be cleaner
# conda deactivate && source ~/sky_workdir/anthropic/bin/activate && ~/sky_workdir/verl/jeff-experiments/run_qwen25_small.sh ~/${FT_MODEL_NAME}
# python ~/sky_workdir/verl/jeff-experiments/lora_merge.py --local_peft_path ~/${FT_MODEL_NAME}/global_step_${FT_TRAINING_STEPS} --local_base_model_path ${REF_MODEL} --output_path ~/${FT_MODEL_NAME}
run: |
  set -e

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate

  source ~/.env_anthropic
  python ~/sky_workdir/verl/jeff-experiments/evaluation/eval_orchestrator.py --grid_script_path ~/sky_workdir/cot-decomp/evaluation/utils/grid_results.py --eval_script_path ~/sky_workdir/cot-decomp/evaluation/orchestration/eval_orchestrator.py --eval_json_path $EVAL_JSON_PATH --eval_output_path $EVAL_GRID_PATH --vllm_model /root/$FT_MODEL_NAME --vllm_model_name $FT_MODEL_NAME

  pkill vllm

  rsync -a --progress --compress --mkpath $EVAL_JSON_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/
  rsync -a --progress --compress --mkpath $EVAL_GRID_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/
  rsync -a --progress --compress --mkpath $EVAL_OUTPUT_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/

  rsync -a --progress --compress ~/${FT_MODEL_NAME} cluster1:/workspace/jeffg/