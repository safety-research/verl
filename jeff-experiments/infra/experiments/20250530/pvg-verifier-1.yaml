#  sky launch -c jeff-cotdecomp-8xh100 jeff-experiments/infra/base_job.yaml  --down
resources:
  cloud: runpod
  accelerators: H200:4
  region: US
  disk_size: 4000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

file_mounts:
  ~/.ssh/id_rsa: ~/.ssh/id_rsa
  ~/.ssh/config: ~/.ssh/config
  ~/.netrc: ~/.netrc
  ~/.cache/huggingface/token: ~/.cache/huggingface/token
  ~/sky_workdir/verl: ~/verl
  ~/sky_workdir/cot-decomp: ~/cot-decomp
  ~/.env_anthropic: ~/.env_anthropic
  # CHANGE ME
  ~/data/raw_prompts/train.parquet: /Users/jeff/cot-decomp/output/loan_dataset_pvg0/train_dataset.parquet
  ~/data/raw_prompts/val.parquet: /Users/jeff/cot-decomp/output/loan_dataset_pvg0/test_dataset.parquet

# working dir is ~/sky_workdir/

envs:
  # CHANGE ME
  EXPERIMENT_NAME: verifier1
  OUTPUT_DIR: prover1
  PROJECT_NAME: pvg-verifier
  REF_MODEL: Qwen/Qwen2.5-7B
  REF_ROLLOUT_MODEL: /root/sky_workdir/checkpoints/pvg/prover0/global_step_147/actor/huggingface
  TRAIN_FILES: /root/data/prover0/val.parquet,/root/data/prover1/val.parquet

setup: |
  set -e
  echo "Running setup."
  chown -R root ~/.ssh
  chmod -R 700 ~/.ssh
  ~/sky_workdir/verl/jeff-experiments/infra/base_setup.sh


# Remember to change lora_merge.py and fsdp_sft_trainer.py to use the correct model loader
# TODO(sguo35): rewrite this to be cleaner



run: |
  set -e

  source ~/.env_anthropic

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate && python ~/sky_workdir/cot-decomp/sft/datagen/loan_dataset/generate_pvg_start.py --model $REF_ROLLOUT_MODEL --prompt-file ~/data/raw_prompts/val.parquet --temperature 1.0 --max-tokens 1024 --tensor-parallel-size 4 --output-file ~/data/$OUTPUT_DIR/val.parquet


  conda deactivate && source ~/sky_workdir/anthropic/bin/activate && ~/sky_workdir/verl/jeff-experiments/run_classifier_sft.sh