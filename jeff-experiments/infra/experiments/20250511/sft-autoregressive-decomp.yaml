#  sky launch -c jeff-cotdecomp-8xh100 jeff-experiments/infra/base_job.yaml  --down
resources:
  cloud: runpod
  accelerators: H200:4
  region: US
  disk_size: 2000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

file_mounts:
  ~/.ssh/id_rsa: ~/.ssh/id_rsa
  ~/.ssh/config: ~/.ssh/config
  ~/.netrc: ~/.netrc
  ~/.cache/huggingface/token: ~/.cache/huggingface/token
  ~/sky_workdir/verl: ~/verl
  ~/sky_workdir/cot-decomp: ~/cot-decomp
  ~/.env_anthropic: ~/.env_anthropic
  # CHANGE ME
  ~/data/train.parquet: /Users/jeff/cot-decomp/output/judge_dataset_autoregressive_mmlu_math_decomp/train.parquet
  ~/data/val.parquet: /Users/jeff/cot-decomp/output/judge_dataset_autoregressive_mmlu_math_decomp/test.parquet

# working dir is ~/sky_workdir/

envs:
  # CHANGE ME
  EXPERIMENT_NAME: sft-decomp-grader-autoregressive
  PROJECT_NAME: grader
  REF_MODEL: Qwen/Qwen2.5-14B
  # CHANGE ME
  FT_TRAINING_STEPS: 202
  # CHANGE ME
  BATCH_SIZE: 32
  MICRO_BATCH_SIZE: 1

setup: |
  set -e
  echo "Running setup."
  chown -R root ~/.ssh
  chmod -R 700 ~/.ssh
  ~/sky_workdir/verl/jeff-experiments/infra/base_setup.sh


# Remember to change lora_merge.py and fsdp_sft_trainer.py to use the correct model loader
# TODO(sguo35): rewrite this to be cleaner

run: |
  set -e

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate && ~/sky_workdir/verl/jeff-experiments/run_qwen_yesno.sh ~/${EXPERIMENT_NAME}
