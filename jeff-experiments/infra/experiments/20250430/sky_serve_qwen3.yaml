# service.yaml
service:
  readiness_probe: /v1/models
  replicas: 1

# Fields below describe each replica.
resources:
  cloud: runpod
  ports: 8080
  accelerators: {H200:4}
  region: US
  disk_size: 2000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

setup: |
  conda create -n vllm python=3.9 -y
  conda activate vllm
  pip install vllm

run: |
  conda activate vllm
  python -m vllm.entrypoints.openai.api_server \
    --enable-reasoning --reasoning-parser deepseek_r1 --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE --max-model-len 32000 --enable-expert-parallel --disable-log-requests --max-num-seqs 1024 \
    --host 0.0.0.0 --port 8080 \
    --model Qwen/Qwen3-235B-A22B-FP8