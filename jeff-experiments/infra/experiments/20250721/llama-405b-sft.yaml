#  sky launch -c jeff-cotdecomp-8xh100 jeff-experiments/infra/base_job.yaml  --down
resources:
  cloud: runpod
  accelerators: H200:8
  region: US
  disk_size: 4000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

file_mounts:
  ~/.ssh/id_rsa: ~/.ssh/id_rsa
  ~/.ssh/config: ~/.ssh/config
  ~/.netrc: ~/.netrc
  ~/.cache/huggingface/token: ~/.cache/huggingface/token
  ~/sky_workdir/verl: ~/verl
  ~/sky_workdir/cot-decomp: ~/cot-decomp
  ~/.env_anthropic: ~/.env_anthropic

# working dir is ~/sky_workdir/

envs:
  # CHANGE ME
  EXPERIMENT_NAME: llama-405b-prompt-lora-sft
  PROJECT_NAME: prompt-elicitation-sft
  REF_MODEL: meta-llama/Llama-3.1-405B-Instruct
  # CHANGE ME
  FT_MODEL_NAME: Llama-3.1-405B-Prompt-SFT
  # CHANGE ME
  BATCH_SIZE: 16
  MICRO_BATCH_SIZE: 1

  LR: 2e-6
  CLIP_GRAD: 1.0

  COT_DECOMP_ROOT: /root/sky_workdir/cot-decomp

setup: |
  set -e
  echo "Running setup."
  chown -R root ~/.ssh
  chmod -R 700 ~/.ssh
  ~/sky_workdir/verl/jeff-experiments/infra/base_setup.sh

run: |
  set -e

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate && ~/sky_workdir/verl/jeff-experiments/run_llama405b_sft.sh /root/${FT_MODEL_NAME}

  python ~/sky_workdir/verl/jeff-experiments/lora_merge.py --local_peft_path /root/Llama-3.1-405B-Prompt-SFT/global_step_95 --local_base_model_path meta-llama/Llama-3.1-405B-Instruct --output_path /root/Llama-3.1-405B-Prompt-SFT/global_step_95