#  sky launch -c jeff-cotdecomp-8xh100 jeff-experiments/infra/base_job.yaml  --down
resources:
  cloud: runpod
  accelerators: H100-SXM:8
  region: US
  disk_size: 2000
  image_id: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04

file_mounts:
  ~/.ssh/id_rsa: ~/.ssh/id_rsa
  ~/.ssh/config: ~/.ssh/config
  ~/.netrc: ~/.netrc
  ~/.cache/huggingface/token: ~/.cache/huggingface/token
  ~/sky_workdir/verl: ~/verl
  ~/sky_workdir/cot-decomp: ~/cot-decomp
  ~/.env_anthropic: ~/.env_anthropic
  ~/data/train.parquet: /Users/jeff/cot-decomp/output/sft_mmlu_math_numina/train.parquet
  ~/data/val.parquet: /Users/jeff/cot-decomp/output/sft_mmlu_math_numina/test.parquet

# working dir is ~/sky_workdir/

envs:
  EXPERIMENT_NAME: mmlu-math-sft-qwen14b-instruct
  PROJECT_NAME: mmlu-sft
  REF_MODEL: Qwen/Qwen2.5-14B-Instruct
  FT_MODEL_NAME: Qwen2.5-14B-Instruct-MMLU-MATH-NUMINA-SFT
  FT_TRAINING_STEPS: 352 # TODO(sguo35): change this
  BATCH_SIZE: 128
  MICRO_BATCH_SIZE: 16

  EVAL_JSON_PATH: /root/sky_workdir/cot-decomp/experiments/notebooks/20250429-jeff/qwen14b.json
  EVAL_GRID_PATH: /root/sky_workdir/cot-decomp/experiments/notebooks/20250429-jeff/qwen14b_mmlu_math_numina_sft_grid_results.json
  EVAL_OUTPUT_PATH: /root/sky_workdir/eval-output

setup: |
  set -e
  echo "Running setup."
  chown -R root ~/.ssh
  chmod -R 700 ~/.ssh
  ~/sky_workdir/verl/jeff-experiments/infra/base_setup.sh


# Remember to change lora_merge.py and fsdp_sft_trainer.py to use the correct model loader
# TODO(sguo35): rewrite this to be cleaner

run: |
  set -e

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate

  conda deactivate && source ~/sky_workdir/anthropic/bin/activate && ~/sky_workdir/verl/jeff-experiments/run_qwen25_full.sh ~/${FT_MODEL_NAME}

  source ~/.env_anthropic
  python ~/sky_workdir/verl/jeff-experiments/evaluation/eval_orchestrator.py --grid_script_path ~/sky_workdir/cot-decomp/evaluation/utils/grid_results.py --eval_script_path ~/sky_workdir/cot-decomp/evaluation/orchestration/eval_orchestrator.py --eval_json_path $EVAL_JSON_PATH --eval_output_path $EVAL_GRID_PATH --vllm_model /root/$FT_MODEL_NAME/global_step_${FT_TRAINING_STEPS} --vllm_model_name $FT_MODEL_NAME

  (pkill vllm || true)

  rsync  -e 'ssh -o "StrictHostKeyChecking no"' -a --progress --compress --mkpath $EVAL_JSON_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/
  rsync  -e 'ssh -o "StrictHostKeyChecking no"' -a --progress --compress --mkpath $EVAL_GRID_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/
  rsync  -e 'ssh -o "StrictHostKeyChecking no"' -a --progress --compress --mkpath $EVAL_OUTPUT_PATH cluster1:/workspace/jeffg/$PROJECT_NAME/$EXPERIMENT_NAME/

  rsync -e 'ssh -o "StrictHostKeyChecking no"' -a --progress --compress ~/${FT_MODEL_NAME} cluster1:/workspace/jeffg/