data:
  tokenizer: null
  train_files: /n/home05/sqin/wall/verl/data/math/train.parquet
  val_files: /n/home05/sqin/wall/verl/data/math/test.parquet
  prompt_key: prompt
  max_prompt_length: 512
  max_response_length: 4096
  train_batch_size: 1024
  val_batch_size: 1312
  return_raw_chat: False
  shuffle: True

actor_rollout_ref:
  model:
    path: /n/netscratch/dam_lab/Lab/sqin/models/qwen/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306 
    enable_gradient_checkpointing: True
    use_remove_padding: True
  actor:
    ppo_mini_batch_size: 256
    ppo_micro_batch_size_per_gpu: 32
    use_dynamic_bsz: True
    ppo_max_token_len_per_gpu: 16384
    grad_clip: 1.0
    clip_ratio: 0.2
    entropy_coeff: 0.001
    use_kl_loss: True
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    ppo_epochs: 1
    optim:
      lr: 1e-6
      lr_warmup_steps_ratio: 0.
      min_lr_ratio: null
      warmup_style: constant
      total_training_steps: -1
    fsdp_config:
      param_offload: False
      grad_offload: False
      optimizer_offload: False
  ref:
    fsdp_config:
      param_offload: True
  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1
    top_p: 1
    prompt_length: 512
    response_length: 2048
    dtype: bfloat16
    gpu_memory_utilization: 0.6
    tensor_model_parallel_size: 2
    max_num_batched_tokens: 8192
    max_num_seqs: 1024
    log_prob_micro_batch_size_per_gpu: 32
    log_prob_use_dynamic_bsz: True
    log_prob_max_token_len_per_gpu: 16384
    disable_log_stats: True
    enable_chunked_prefill: True
    do_sample: True
    n: 8

algorithm:
  kl_ctrl:
    kl_coef: 0.001
  adv_estimator: grpo

trainer:
  total_epochs: 20
  project_name: reason-wall
  experiment_name: Qwen2.5-1.5B-Instruct_verl_grpo_MATH_train_dummy
  logger: [ 'console', 'wandb' ]
  val_before_train: False
  nnodes: 1
  n_gpus_per_node: 2
  save_freq: 5
  test_freq: 1
  critic_warmup: 0
  default_local_dir: /n/netscratch/dam_lab/Lab/sqin/wall/checkpoints/${trainer.experiment_name}
