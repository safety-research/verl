    近年来大模型的参数规模不断增加，所需的算力急剧增长，从 GPT-1 到 GPT-3，再到类似 GPT-4 的 GPT MOE 1.8T，算力需求增长了数万倍。支持模型规模和算力需求增长的是硬件算力的提升。低精度正在成为大模型训练和推理中的一个趋势。因为相比于bf16/fp16，FP8/FP6/FP4 显著降低显存占用和提升计算吞吐。相较 BF16，FP8 的矩阵乘法峰值性能能够实现翻倍，FP8权重存储是BF16权重存储的50%。相较于BF16，FP8表示的范围和精度较低，对模型训练效果有影响，如：grad 出现 Inf/NaN，loss 出现spike；训练 loss 正常，产出模型测评得分偏低；产出模型测评正常，使用FP8训练的加速比达不到预期。
    为了能够在有效时间内训练产出大模型，充分利用硬件算力，各个基座模型团队均在尝试使用低精度训练，解决低精度训练过程可能带来的问题。业界探究了多种方法来解决低精度的问题，NV提出了dynamic scheduling，DeepSeek 团队提出的对于激活值采用tile wise量化以及权重提出 block-wise 量化方法，并在实践中取得了成功。
    目前关于FP8低精度训练主要是应用在预训练。强化学习涉及样本生成和训练两个阶段，为了充分挖掘模型推理能力，生成的序列长度会16K甚至32K，如果可以采用FP8训练（权重）和推理（权重和kv-cache），那么可以获取非常显著的性能提提升。在强化学习训练领域尚无使用FP8训练和推理的方法介绍。 Seed-Thinking-v1.5 介绍了使用在和环境交互过程中使用FP8策略模型，但是未提及训练过程是否使用了FP8。强化学习训练中使用FP8也会存在挑战：如果训练和推理使用的量化方式不同，导致训练模型和rollout模型存在精度误差，是否会存在精度误差，影响模型训练；强化学习场景的FP8训练的动力学，如何保证量化精度损失最低，如何选择前向反向数值格式；硬件仅支持矩阵乘法 nt 格式，需要对输入进行转置，以及满足对齐要求对数据进行pad，小算子比较多，产生 kernal launch bound情况，是否能够获取预期的加速。
    为了验证 FP8 训练和推理是否会对强化学习效果产生影响，我们在 verl 库基础上，基于 DeepGEMM 构建了 FP8 Linear，在前向传播过程中对于权重进行 block-wise 量化，对于激活值采用 tile-wise 量化。基线训练配置是 BF16 训练，权重包括 BF16 权重和 FP32 权重副本；FP8 训练是指对于模型中的 torch.nn.Linear 部分替换为 FP8 Linear，其他部分仍然采用 BF16（gating, rms_norm, embedding）。使用 vllm 用于样本生成，基线配置是 BF16 权重推理，FP8 推理采用了 Online Dynamic Quantization，对每个 tensor 进行量化。选择了一个基线任务（qwen3 gsm 8K），对训练和推理采用不同的精度组合（BF16-BF16，BF16-FP8，FP8-FP8），并且对比了模型训练过程中的评估指标，评估指标如下表所示，使用FP8训练和推理不会影响强化学习训练效果。

    加速情况：当前FP8 Linear 是基于 DeepGeem 提供的前反向算子构建的 Linear Module，需要对输入进行转置，以及满足对齐要求对数据进行pad，小算子比较多，存在额外开销导致训练并未产生加速。权重同步方面，由于训练和推理量化方式不同，每次同步权重时仍然需要基于训练的 BF16 权重进行量化，导致权重同步阶段额外开销比较大，但是采用 FP8 的推理的吞吐比 BF16 高。和推理可以采用相同的量化方式，减少权重同步时的开销，做到训练和推理一体设计。

Related work
transformer engine
DeepGemm
Seed-Thinking-v1.5




 
